{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Pré-processamento"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "G1UPjVuhCIbW",
    "outputId": "22418a9e-7744-43cb-d8f1-f9651d11f0d9"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     /home/adrielson/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "from unicodedata import normalize\n",
    "import pandas as pd\n",
    "import re\n",
    "import regex\n",
    "import nltk\n",
    "nltk.download('stopwords')\n",
    "\n",
    "#Plotagem\n",
    "%matplotlib inline \n",
    "import matplotlib.pyplot as plt \n",
    "from wordcloud import WordCloud \n",
    "from wordcloud import WordCloud "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "id": "lf6_z0QZEQBz"
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Unnamed: 0.1</th>\n",
       "      <th>Unnamed: 0</th>\n",
       "      <th>id</th>\n",
       "      <th>title</th>\n",
       "      <th>text</th>\n",
       "      <th>humanLanguage</th>\n",
       "      <th>pageUrl</th>\n",
       "      <th>requirements</th>\n",
       "      <th>tasks</th>\n",
       "      <th>word_count</th>\n",
       "      <th>language</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>JOB13030080891</td>\n",
       "      <td>Social Media &amp; Web Management Internship</td>\n",
       "      <td>\\nIntroduction\\nThe Social Media &amp; Web Managem...</td>\n",
       "      <td>en</td>\n",
       "      <td>https://www.nps.gov/hafe/getinvolved/supportyo...</td>\n",
       "      <td>If not selected for this internship, please in...</td>\n",
       "      <td>Assist in creating and managing social media (...</td>\n",
       "      <td>501</td>\n",
       "      <td>en</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   Unnamed: 0.1  Unnamed: 0              id  \\\n",
       "0             0           0  JOB13030080891   \n",
       "\n",
       "                                      title  \\\n",
       "0  Social Media & Web Management Internship   \n",
       "\n",
       "                                                text humanLanguage  \\\n",
       "0  \\nIntroduction\\nThe Social Media & Web Managem...            en   \n",
       "\n",
       "                                             pageUrl  \\\n",
       "0  https://www.nps.gov/hafe/getinvolved/supportyo...   \n",
       "\n",
       "                                        requirements  \\\n",
       "0  If not selected for this internship, please in...   \n",
       "\n",
       "                                               tasks  word_count language  \n",
       "0  Assist in creating and managing social media (...         501       en  "
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = pd.read_csv('df_english_only.csv')\n",
    "df.head(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "7273"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "def pre_processing(text):\n",
    "    from nltk.corpus import stopwords\n",
    "    stopwords = stopwords.words('english')\n",
    "\n",
    "    #lista com stopwords não removidas pela nltk\n",
    "    stopwords.extend(['see', 'use', 'please', 'send', 'e g', 'us'])\n",
    "        \n",
    "    #adaptação do texto\n",
    "    adp_text = str(text).lower()\n",
    "    #adp_text = re.sub(\"[\\(\\[].*?[\\)\\]]\", \"\", (adp_text))\n",
    "    adp_text = re.sub(r\"\\(([^)]+)\\)\", r\"\\1\", (adp_text)) #remove parenteses e colchetes e mantem conteudo interno\n",
    "    adp_text = adp_text.replace('\\n', '').strip() #remove quebras de linha\n",
    "    adp_text = re.sub(r\"[0-9]+\", \"\", (adp_text)) #remove números\n",
    "    \n",
    "    #remove acentuação\n",
    "    acent_text = normalize('NFKD', adp_text).encode('ASCII','ignore').decode('ASCII')\n",
    "    \n",
    "          \n",
    "    #remove os caracteres listados e espaços excessivos\n",
    "    ponct_text = re.sub(r\"\\.|,|;|:|-|!|\\?|´|`|^|'|#|]|\\[|_|@|=|\\(|\\)| |\\$|~|\\||\\+|<|\\\"|>|&|\\*|/|%\", \" \", acent_text)\n",
    "    \n",
    "    ponct_text = ponct_text.strip()\n",
    "    ponct_text = \" \".join(ponct_text.split())\n",
    "    \n",
    "    #Remove as stopwords de um texto \n",
    "    for sw in stopwords:\n",
    "        #stop_text = re.sub(r'\\b%s\\b' % sw, \"\", acent_text)\n",
    "        stop_text = re.sub(r'\\b(%s)\\b[\\s.,;:?!]' % '|'.join(stopwords), '', ponct_text)\n",
    "\n",
    "    \n",
    "    return stop_text\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "nlp = spacy.load(\"en_core_web_sm\", disable=['parser', 'ner'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "def lemmatization(texts, allowed_postags=['NOUN', 'ADJ', 'VERB', 'ADV']):\n",
    "    \"\"\"https://spacy.io/api/annotation\"\"\"\n",
    "    texts_out = []\n",
    "    for sent in texts:\n",
    "        doc = nlp(\" \".join(sent)) \n",
    "        texts_out.append([token.lemma_ for token in doc if token.pos_ in allowed_postags])\n",
    "    return texts_out"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Do lemmatization keeping only noun, adj, vb, adv\n",
    "data_lemmatized = lemmatization(df['text'], allowed_postags=['NOUN', 'ADJ', 'VERB', 'ADV'])\n",
    "\n",
    "print(data_lemmatized[:1][0][:30])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "def pre_processing(text):\n",
    "    from nltk.corpus import stopwords\n",
    "    stopwords = stopwords.words('english')\n",
    "\n",
    "    #lista com stopwords não removidas pela nltk\n",
    "    stopwords.extend(['http', 'https', 'mythdhrhomedepotcom','amp', 'com','home depot','depot', 'homedepot','job','jobs',\n",
    "                      'less','journalismjobs','clickon find', 'find tab', 'clickon', 'longer', 'longer listed',\n",
    "                      'tab search', 'sorry longer', 'listed journalismjobs','flexjobs find', 'flexjobs',\n",
    "                      'find', 'best flexible', 'jobs fantastic', 'fantastic expert', 'member today', 'joband great',\n",
    "                      'flexjobs find','email', 'mythdhr', 'email mythdhr','hospice','devex','starting devex','please', 'send',\n",
    "                      'include', 'e g','upload','cookies','use','cookie','next', 'xxx','xx', 'xxxx','xxxxxxx','xxxxxx', 'xxxxx',\n",
    "                      'xxxxxxxxx','xxxxxxxx','tab','sorry','contractors','started','agent','registration','contractor',\n",
    "                      'recruiting','select','notificati','remote','correspondences','news', 'headquartered','visitors','founded','covering','engaged',\n",
    "                      'click', 'password','address', 'subscribe','alerts','adsl','paia','criteriaalert','alerted', 'removing','vp', 'inc',\n",
    "                      'found','words', 'common', 'entire', 'employer','full', 'direct','us','mediabistro','bizcommunity','mail','post job',\n",
    "                      'employment', 'cv', 'resumebe', 'copyright','forgotten', 'similar', 'glassdoor', 'popular', 'new','york', 'new', 'jersey',\n",
    "                      'trenton', 'washington', 'chicago','time', 'massachusetts', 'enter','keywordsjob','view','lis',\n",
    "                      'jobtrain', 'afrihost','herein','hong', 'kong','already','account','sign','yet','','brooklyn','county', 'manhattan',\n",
    "                      'york', 'new','per','unique','xxxxxxxxxx', 'xxxxxxxxxxx', 'xxxxxxxxxxxx', 'xxxxxxxxxxxxx','comrecipients', 'mediapost', 'board',\n",
    "                  'required','dreams', 'mashable', 'browse', 'today', 'listings', 'powered', 'kings', 'cook', 'authorized', 'part',\n",
    "                  'following', 'response', 'individuals', 'organization', 'needplease',\n",
    "                  'application', 'seekers', 'accommodation', 'reasonable', 'better','listed', 'try','return', 'contained','madgex',\n",
    "                  'exclusive','career','resultsconnections', 'jobsyour','recruiters','stands', 'quickly', 'expired', 'closed', 'listing', 'hkd', 'menu', 'onwards', 'listings', 'filled',\n",
    "                  'accepting',  'apologies', 'position','notifies','weeklyselect', 'titles','vacancy',  'simply','check', 'supported',\n",
    "                  'delivered', 'applicant', 'limited', 'solutions', 'conditionsinforma', 'conditionsodeon', 'connecticut', 'florida',\n",
    "                  'arizona', 'illinois', 'texas', 'california', 'angeles', 'los', 'melville', 'suffolk',\n",
    "                  'become',  'complete','alert', 'details', 'application', 'get', 'apply',\n",
    "                  'colleague', 'friend', 'form', 'know', 'concerns', 'refer','appear', 'let', 'americaif',\n",
    "                  'linkedingooglefacebooktwitter','titlelocation', 'suiteamazonetsyshopifygoogle',\n",
    "                  'facebookinstagramlinkedinmeetupzoomok', 'cupidmatchmail', 'chimpgmailhoot', 'login','others','premium', 'hand', 'register',\n",
    "                  'free', 'picks', 'members','get', 'join', 'registered', 'picked'\n",
    "                  'miss', 'forgot', 'username','isabilities', 'comin', 'representedthe', 'organizationa',\n",
    "                  'applyif', 'sending', 'thank', 'accommodations', 'sent', 'requested','fox', 'content', 'work', 'nbc',\n",
    "                  'seattle', 'francisco', 'san', 'virginia', 'information','tele', 'telegraph', 'emailalternatively',\n",
    "                  'reload', 'discount', 'code', 'error', 'checkout', 'checkouttake','results', 'containing', 'query', \n",
    "                  'search', 'terms', 'many', 'set', 'specific', 'available', 'result'\n",
    "])#'go to', 'to go','please go', 'go by', 'go on', 'ability go', 'care go', 'need go', 'like go', 'want go',\n",
    "    \n",
    "    # adaptação do texto\n",
    "    adp_text = str(text).lower()\n",
    "    adp_text = re.sub(r\"\\(([^)]+)\\)\", r\"\\1\", adp_text) # remove parenteses e colchetes e mantem conteudo interno\n",
    "    adp_text = adp_text.replace('\\n', '').strip() # remove quebras de linha\n",
    "    \n",
    "    # remove acentuação\n",
    "    acent_text = normalize('NFKD', adp_text).encode('ASCII','ignore').decode('ASCII')\n",
    "    \n",
    "    # remove os caracteres listados e espaços excessivos\n",
    "    ponct_text = re.sub(r\"\\.|,|;|:|-|!|\\?|´|`|^|'|]|\\[|_|@|=|\\(|\\)| |\\$|~|\\||<|\\\"|>|&|\\*|/|%\", \" \", acent_text)\n",
    "    \n",
    "    # Remove URLs\n",
    "    url_removed_text = re.sub(r'http\\S+|www\\S+', '', ponct_text)\n",
    "\n",
    "    # Remove stopwords from the text\n",
    "    stop_text = url_removed_text\n",
    "    for sw in stopwords:\n",
    "        stop_text = re.sub(r'\\b%s\\b' % sw, '', stop_text)\n",
    "        stop_text = stop_text.replace('  ', ' ') # Adiciona um espaço em branco após cada remoção de stop word\n",
    "    \n",
    "    return stop_text\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "df['text_result'] = df['text'].apply(pre_processing)\n",
    "# df['title_result'] = df['title'].apply(pre_processing)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#criei essa função para verificar os caracteres na coluna text\n",
    "def verificar_caracteres(df):\n",
    "    caracteres = []\n",
    "    \n",
    "    for row in df['text_result']:\n",
    "        caracteres += re.findall(r'[^a-zA-Z0-9]', row)\n",
    "    \n",
    "    caracteres = list(set(caracteres))\n",
    "    \n",
    "    return caracteres"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "print(verificar_caracteres(df))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0     introductionthe social media web management i...\n",
       "1     usthirty three threads industry leader high p...\n",
       "2     web manager experienced interactive team hous...\n",
       "3     high affinity life sciencesfull 40 hrs playin...\n",
       "4     core social team responsible post main brande...\n",
       "5     mars reelmars reel fast growing mobile sports...\n",
       "6     high affinity life sciencesfull 40 hrs playin...\n",
       "7     fun train vr company believes virtual reality...\n",
       "8     actually mean go 2014 posting one 2011 post s...\n",
       "9     title objective maintain clubs social media f...\n",
       "Name: text_result, dtype: object"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df2.text_result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def nuvem_palavras(dados, coluna_alvo):  \n",
    "    todas_palavras = ' '.join([str(texto) for texto in dados[coluna_alvo]]) \n",
    "    \n",
    "\n",
    "    nuvem_palavras = WordCloud(width= 800, height= 500, max_words=100,\n",
    "                             max_font_size = 110, background_color=\"white\").generate(todas_palavras)\n",
    "\n",
    "\n",
    "    plt.figure(figsize=(10,7))\n",
    "    plt.imshow(nuvem_palavras, interpolation='bilinear')\n",
    "    plt.axis(\"off\")\n",
    "    plt.show "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "nuvem_palavras(df, \"text_result\") "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#30 antes e depois\n",
    "\n",
    "window = 50\n",
    "\n",
    "caracter = 'html'\n",
    "for index, row in df.iterrows():\n",
    "    a = row.text_result\n",
    "    n = -1\n",
    "    if(caracter in a):\n",
    "        n = a.find(caracter)\n",
    "        start_index = max(0, n - window)\n",
    "        end_index = min(len(a), n + window)\n",
    "        s = a[start_index:end_index]\n",
    "        print(index, s, '\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.to_csv('vagas_social_midia_preprocessado3.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
